{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\pytorch-CycleGAN-and-pix2pix\\lib\\site-packages\\ipykernel_launcher.py:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import gensim\n",
    "import pyLDAvis.gensim as gensimvis\n",
    "import pyLDAvis\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from tokenizer.korean_tokenizer import tokenize_nori,tokenize_okt,tokenize_okt_noscreen\n",
    "from gensim.models import LdaModel\n",
    "from os.path import join\n",
    "from os import remove\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n",
    "from render import korean_render,english_render\n",
    "from tokenizer.english_tokenizer import tokenize_nltk\n",
    "tqdm_notebook.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "donga.xlsx file visualize start\n",
      "Tokenize Start\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1417.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb323c80e4894129913703d47f7fb4cf"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "pos() got an unexpected keyword argument 'stem'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-8ae443d255ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mtemp_screen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvisualize_korean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtokenize_okt_noscreen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'./visualize/korean_visualize/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.xlsx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/unscreen'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeyword_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mtemp_unscreen\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mvisualize_korean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtokenize_okt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'./visualize/korean_visualize/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.xlsx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/screen'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeyword_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{file} file visualize end'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-7d796628a61b>\u001b[0m in \u001b[0;36mvisualize_korean\u001b[1;34m(df, tokenizer, root_path, topic_num, keyword_num, topn)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tokenize Start'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tokenize End'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mdetoken_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'detokenize_text.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\visualize_kaist-test_visualize\\tokenizer\\korean_tokenizer.py\u001b[0m in \u001b[0;36mtokenize_okt_noscreen\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[0mpostprocessor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPostprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mokt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     df['content_token'] = df.progress_apply(lambda x: [t[0] for t in postprocessor.pos(\n\u001b[1;32m--> 122\u001b[1;33m         x['content'], stem=True) if t[1] in ['Noun', 'Verb', 'Adjective']], axis=1)\n\u001b[0m\u001b[0;32m    123\u001b[0m     df['title_token'] = df.progress_apply(lambda x: [t[0] for t in postprocessor.pos(\n\u001b[0;32m    124\u001b[0m         x['title'], stem=True) if t[1] in ['Noun', 'Verb', 'Adjective']], axis=1)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch-CycleGAN-and-pix2pix\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    795\u001b[0m                 \u001b[1;31m# on the df using our wrapper (which provides bar updating)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 797\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    798\u001b[0m                 \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch-CycleGAN-and-pix2pix\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[0;32m   7546\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7547\u001b[0m         )\n\u001b[1;32m-> 7548\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7550\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DataFrame\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch-CycleGAN-and-pix2pix\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch-CycleGAN-and-pix2pix\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m         \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[1;31m# wrap results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch-CycleGAN-and-pix2pix\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                     \u001b[1;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                         \u001b[1;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch-CycleGAN-and-pix2pix\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    790\u001b[0m                     \u001b[1;31m# take a fast or slow code path; so stop when t.total==t.n\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m                 \u001b[1;31m# Apply the provided function (in **kwargs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\visualize_kaist-test_visualize\\tokenizer\\korean_tokenizer.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[0mpostprocessor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPostprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mokt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     df['content_token'] = df.progress_apply(lambda x: [t[0] for t in postprocessor.pos(\n\u001b[1;32m--> 122\u001b[1;33m         x['content'], stem=True) if t[1] in ['Noun', 'Verb', 'Adjective']], axis=1)\n\u001b[0m\u001b[0;32m    123\u001b[0m     df['title_token'] = df.progress_apply(lambda x: [t[0] for t in postprocessor.pos(\n\u001b[0;32m    124\u001b[0m         x['title'], stem=True) if t[1] in ['Noun', 'Verb', 'Adjective']], axis=1)\n",
      "\u001b[1;31mTypeError\u001b[0m: pos() got an unexpected keyword argument 'stem'"
     ]
    }
   ],
   "source": [
    "korean_df_folder = join('korean_crawling','articles')\n",
    "korean_df_screen = pd.DataFrame()\n",
    "korean_df_unscreen = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir(korean_df_folder):\n",
    "    if file.endswith(\".xlsx\"):\n",
    "        print(f'{file} file visualize start')\n",
    "        temp_df = pd.read_excel(join(korean_df_folder,file))\n",
    "        try:\n",
    "            temp_df.rename(columns={\"urls\" : \"url\",\"titles\":\"title\",\"contents\":\"content\"},inplace=True)\n",
    "            temp_df=temp_df.drop(['Unnamed: 0'],axis=1)\n",
    "        except:\n",
    "            pass\n",
    "        temp_screen=visualize_korean(temp_df,tokenize_okt_noscreen,'./visualize/korean_visualize/'+file.rstrip('.xlsx')+'/unscreen',keyword_num=60)\n",
    "        temp_unscreen= visualize_korean(temp_df,tokenize_okt,'./visualize/korean_visualize/'+file.rstrip('.xlsx')+'/screen',keyword_num=60)\n",
    "        print(f'{file} file visualize end')\n",
    "        korean_df_screen=pd.concat([korean_df_screen, temp_screen])\n",
    "        korean_df_unscreen = pd.concat([korean_df_unscreen,temp_unscreen])\n",
    "        \n",
    "visualize_korean(korean_df_screen,None,'./visualize/korean_visualize/'+'merge'+'/screen',keyword_num=60)\n",
    "visualize_korean(korean_df_unscreen,None,'./visualize/korean_visualize/'+'merge'+'/unscreen',keyword_num=60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tokenize Start\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=31.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ce36a83947e40dfbcde14406cd0bb62"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=31.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7eea5721fb904b7e8b839ddc7ff1a9c9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "tokenize End\n",
      "detokenize end\n",
      "topic model made\n",
      "word2vec end\n",
      "made visualize file\n",
      "made visualize file\n",
      "Tokenize Start\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=84.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e30d3ad3d4f5481e82c394d87bb2273d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=84.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b43e63d48b144ce98f2e2fbe618cd872"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "tokenize End\n",
      "detokenize end\n",
      "topic model made\n",
      "word2vec end\n",
      "made visualize file\n",
      "made visualize file\n",
      "Tokenize Start\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=19.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb441f8820cb42789cfb8d817cf865bd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=19.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "92f210f88a5c4d02b3eb9ebe203c08da"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "tokenize End\n",
      "detokenize end\n",
      "topic model made\n",
      "word2vec end\n",
      "made visualize file\n",
      "made visualize file\n",
      "Tokenize Start\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=18.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7058aea5ae414e5ea7bfed1b17893615"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=18.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e980e1ac554848559ed27a40141b5535"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "tokenize End\n",
      "detokenize end\n",
      "topic model made\n",
      "word2vec end\n",
      "made visualize file\n",
      "made visualize file\n",
      "Tokenize Start\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=99.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9c9d90ec0e04420f950655efcefae975"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=99.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f2347b54b014aa78ca357940511d01f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "tokenize End\n",
      "detokenize end\n",
      "topic model made\n",
      "word2vec end\n",
      "made visualize file\n",
      "made visualize file\n",
      "Tokenize Start\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=165.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88c12e10eb844217be40dab6133ccdc9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=165.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f82eabaf30514a3c8b616eac45c22058"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "tokenize End\n",
      "detokenize end\n",
      "topic model made\n",
      "word2vec end\n",
      "made visualize file\n",
      "made visualize file\n",
      "Tokenize Start\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=38.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "318547c91c95409483c9557172d8e818"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=38.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dc72b22e5bac4643b363e3fc425e5142"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "tokenize End\n",
      "detokenize end\n",
      "topic model made\n",
      "word2vec end\n",
      "made visualize file\n",
      "made visualize file\n",
      "Tokenize Start\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=105.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "798e0b17b9eb4efd86dbcaae7e3a3e7f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=105.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "780246a288f6488faa193ad8ba5f82ed"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "tokenize End\n",
      "detokenize end\n",
      "topic model made\n",
      "word2vec end\n",
      "made visualize file\n",
      "made visualize file\n",
      "Tokenize Start\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=97.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b0d9924151774dfabce54de42ed36548"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=97.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d56c6c485ca4691b7f34be8a7680cae"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "tokenize End\n",
      "detokenize end\n",
      "topic model made\n",
      "word2vec end\n",
      "made visualize file\n",
      "made visualize file\n",
      "Tokenize Start\n",
      "tokenize End\n",
      "detokenize end\n",
      "topic model made\n",
      "word2vec end\n",
      "made visualize file\n",
      "made visualize file\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              content  \\\n",
       "0   Studies have already shown that by irradiating...   \n",
       "1   January 25, 2005  A humanoid robot developed b...   \n",
       "2   Make no mistake, we are in the middle of a pro...   \n",
       "3   A lot has been made of the dangers of drones m...   \n",
       "4   If you build a soft, conformable robot body bu...   \n",
       "..                                                ...   \n",
       "92  GS Caltex and the Korea Advanced Institute of ...   \n",
       "93  Hynix Semiconductor disclosed on August 4 that...   \n",
       "94  The South Korean government unveiled revision ...   \n",
       "95   When I visited Seoul in 2004, for a conferenc...   \n",
       "96  Maeil Business Newspaper (MBN) is found as a b...   \n",
       "\n",
       "                                                title  \\\n",
       "0               Wearable LED device could regrow hair   \n",
       "1   Korea shows its robotics prowess with a rival ...   \n",
       "2   From weapons to works of art: The year in arti...   \n",
       "3   Flying sheepdogs: Herding birds away from plan...   \n",
       "4   Thin-strip \"muscles\" may find use in soft n' s...   \n",
       "..                                                ...   \n",
       "92  S. Korea Develops Technology to Mass Produce B...   \n",
       "93  Hynix Appoints Kim Ji-bum for New Chief Market...   \n",
       "94  Sejong City to Become City of Education, Scien...   \n",
       "95  G20: An Opportunity to Showcase Korea`s Successes   \n",
       "96  Maeil Business News Preferred Most by S. Korea...   \n",
       "\n",
       "                                                  url  \\\n",
       "0          https://newatlas.com/led-hair-array/56306/   \n",
       "1   https://newatlas.com/korea-shows-its-robotics-...   \n",
       "2   https://newatlas.com/year-highlights-artificia...   \n",
       "3   https://newatlas.com/flying-sheepdogs-drones-p...   \n",
       "4   https://newatlas.com/artificial-muscles-soft-r...   \n",
       "..                                                ...   \n",
       "92  https://pulsenews.co.kr/view.php?year=2009&no=...   \n",
       "93  https://pulsenews.co.kr/view.php?year=2009&no=...   \n",
       "94  https://pulsenews.co.kr/view.php?year=2010&no=...   \n",
       "95  https://pulsenews.co.kr/view.php?year=2010&no=...   \n",
       "96  https://pulsenews.co.kr/view.php?year=2009&no=...   \n",
       "\n",
       "                                        content_token  \\\n",
       "0   [study, show, irradiate, bald, skin, use, red,...   \n",
       "1   [january, humanoid, robot, develop, korea, adv...   \n",
       "2   [make, mistake, middle, profoundly, significan...   \n",
       "3   [lot, danger, drone, mix, aircraft, team, engi...   \n",
       "4   [build, soft, conformable, robot, body, rigid,...   \n",
       "..                                                ...   \n",
       "92  [caltex, korea, advance, institute, science, t...   \n",
       "93  [hynix, semiconductor, disclose, august, name,...   \n",
       "94  [the, south, korean, government, unveil, revis...   \n",
       "95  [when, visit, seoul, conference, university, h...   \n",
       "96  [maeil, business, newspaper, mbn, find, busine...   \n",
       "\n",
       "                                          title_token  \n",
       "0              [wearable, lead, device, regrow, hair]  \n",
       "1            [korea, robotics, prowess, rival, asimo]  \n",
       "2   [from, weapon, art, the, artificial, intellige...  \n",
       "3   [fly, sheepdog, herd, bird, plane, autonomous,...  \n",
       "4           [thinstrip, muscle, soft, squishy, robot]  \n",
       "..                                                ...  \n",
       "92  [korea, develop, technology, mass, produce, bi...  \n",
       "93  [hynix, appoint, kim, jibum, new, chief, marke...  \n",
       "94  [sejong, city, become, city, education, scienc...  \n",
       "95            [opportunity, showcase, korea, success]  \n",
       "96  [maeil, business, news, prefer, most, korean, ...  \n",
       "\n",
       "[656 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content</th>\n      <th>title</th>\n      <th>url</th>\n      <th>content_token</th>\n      <th>title_token</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Studies have already shown that by irradiating...</td>\n      <td>Wearable LED device could regrow hair</td>\n      <td>https://newatlas.com/led-hair-array/56306/</td>\n      <td>[study, show, irradiate, bald, skin, use, red,...</td>\n      <td>[wearable, lead, device, regrow, hair]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>January 25, 2005  A humanoid robot developed b...</td>\n      <td>Korea shows its robotics prowess with a rival ...</td>\n      <td>https://newatlas.com/korea-shows-its-robotics-...</td>\n      <td>[january, humanoid, robot, develop, korea, adv...</td>\n      <td>[korea, robotics, prowess, rival, asimo]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Make no mistake, we are in the middle of a pro...</td>\n      <td>From weapons to works of art: The year in arti...</td>\n      <td>https://newatlas.com/year-highlights-artificia...</td>\n      <td>[make, mistake, middle, profoundly, significan...</td>\n      <td>[from, weapon, art, the, artificial, intellige...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A lot has been made of the dangers of drones m...</td>\n      <td>Flying sheepdogs: Herding birds away from plan...</td>\n      <td>https://newatlas.com/flying-sheepdogs-drones-p...</td>\n      <td>[lot, danger, drone, mix, aircraft, team, engi...</td>\n      <td>[fly, sheepdog, herd, bird, plane, autonomous,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>If you build a soft, conformable robot body bu...</td>\n      <td>Thin-strip \"muscles\" may find use in soft n' s...</td>\n      <td>https://newatlas.com/artificial-muscles-soft-r...</td>\n      <td>[build, soft, conformable, robot, body, rigid,...</td>\n      <td>[thinstrip, muscle, soft, squishy, robot]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>GS Caltex and the Korea Advanced Institute of ...</td>\n      <td>S. Korea Develops Technology to Mass Produce B...</td>\n      <td>https://pulsenews.co.kr/view.php?year=2009&amp;no=...</td>\n      <td>[caltex, korea, advance, institute, science, t...</td>\n      <td>[korea, develop, technology, mass, produce, bi...</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>Hynix Semiconductor disclosed on August 4 that...</td>\n      <td>Hynix Appoints Kim Ji-bum for New Chief Market...</td>\n      <td>https://pulsenews.co.kr/view.php?year=2009&amp;no=...</td>\n      <td>[hynix, semiconductor, disclose, august, name,...</td>\n      <td>[hynix, appoint, kim, jibum, new, chief, marke...</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>The South Korean government unveiled revision ...</td>\n      <td>Sejong City to Become City of Education, Scien...</td>\n      <td>https://pulsenews.co.kr/view.php?year=2010&amp;no=...</td>\n      <td>[the, south, korean, government, unveil, revis...</td>\n      <td>[sejong, city, become, city, education, scienc...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>When I visited Seoul in 2004, for a conferenc...</td>\n      <td>G20: An Opportunity to Showcase Korea`s Successes</td>\n      <td>https://pulsenews.co.kr/view.php?year=2010&amp;no=...</td>\n      <td>[when, visit, seoul, conference, university, h...</td>\n      <td>[opportunity, showcase, korea, success]</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>Maeil Business Newspaper (MBN) is found as a b...</td>\n      <td>Maeil Business News Preferred Most by S. Korea...</td>\n      <td>https://pulsenews.co.kr/view.php?year=2009&amp;no=...</td>\n      <td>[maeil, business, newspaper, mbn, find, busine...</td>\n      <td>[maeil, business, news, prefer, most, korean, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>656 rows × 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "english_df = pd.DataFrame()\n",
    "for file in os.listdir(join('english_crawling','result')):\n",
    "    if file.endswith('.csv'):\n",
    "        temp_df = pd.read_csv(join('english_crawling','result',file))\n",
    "        try:\n",
    "            temp_df.rename(columns={\"urls\" : \"url\",\"titles\":\"title\",\"contents\":\"content\"},inplace=True)\n",
    "        except :\n",
    "            pass\n",
    "        try:\n",
    "            temp_unscreen =visualize_english(temp_df,tokenize_nltk,'./visualize/english_visualize/'+file.rstrip('.csv'))\n",
    "            english_df = pd.concat([english_df,temp_unscreen])\n",
    "        except:\n",
    "            continue\n",
    "visualize_english(english_df,None,'./visualize/english_visualize/merge',keyword_num=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tokenize Start\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=629.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4098ff4a68f6491aba56c49cc08541cf"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=629.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "06c56e93d60c4116977aa18cbae31df7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "tokenize End\n",
      "detokenize end\n",
      "topic model made\n",
      "word2vec end\n",
      "made visualize file\n",
      "made visualize file\n",
      "Tokenize Start\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=52.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57a974882d1f4bf5838650ac0b1eb8ef"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=52.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce7e4106d61a42b28dddb7600f324fe8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "tokenize End\n",
      "detokenize end\n",
      "topic model made\n",
      "word2vec end\n",
      "made visualize file\n",
      "made visualize file\n",
      "Tokenize Start\n",
      "tokenize End\n",
      "detokenize end\n",
      "topic model made\n",
      "word2vec end\n",
      "made visualize file\n",
      "made visualize file\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              content  \\\n",
       "0   “Good ideas come to me when I am very comforta...   \n",
       "1   The water/metal interface often governs import...   \n",
       "2   In using nanostructures to design solar therma...   \n",
       "3   Although glucose-sensing neurons were identifi...   \n",
       "4   In tomographic reconstruction, the image quali...   \n",
       "..                                                ...   \n",
       "47  Many essential biological processes are contro...   \n",
       "48  Protection of endothelial integrity has been r...   \n",
       "49  Organicinorganic hybrid perovskites are emergi...   \n",
       "50  Capturing CO2 from humid flue gases and atmosp...   \n",
       "51  During intracellular membrane trafficking Neth...   \n",
       "\n",
       "                                                title  \\\n",
       "0                                          Won Do Heo   \n",
       "1   Structure, Dynamics, and Wettability of Water ...   \n",
       "2   Design of a Broadband Solar Thermal Absorber U...   \n",
       "3   A glucose-sensing neuron pair regulates insuli...   \n",
       "4   Deep learning-based optical field screening fo...   \n",
       "..                                                ...   \n",
       "47  A chemical biology route to site-specific auth...   \n",
       "48  Amelioration of sepsis by TIE2 activation–indu...   \n",
       "49  Overcoming the electroluminescence efficiency ...   \n",
       "50  CO2 capture from humid flue gases and humid at...   \n",
       "51  Spring-loaded unraveling of a single SNARE com...   \n",
       "\n",
       "                                                  url  \\\n",
       "0   https://www.nature.com/articles/s41592-019-0626-1   \n",
       "1   https://www.nature.com/articles/s41598-019-513...   \n",
       "2   https://www.nature.com/articles/s41598-019-514...   \n",
       "3   https://www.nature.com/articles/s41586-019-1675-4   \n",
       "4   https://www.nature.com/articles/s41598-019-513...   \n",
       "..                                                ...   \n",
       "47  https://science.sciencemag.org/content/354/631...   \n",
       "48   https://stm.sciencemag.org/content/8/335/335ra55   \n",
       "49  https://science.sciencemag.org/content/350/626...   \n",
       "50  https://science.sciencemag.org/content/350/625...   \n",
       "51  https://science.sciencemag.org/content/347/622...   \n",
       "\n",
       "                                        content_token  \\\n",
       "0   [good, idea, comfortable, happy, win, heo, bio...   \n",
       "1   [the, watermetal, interface, govern, chemophys...   \n",
       "2   [use, nanostructures, design, solar, thermal, ...   \n",
       "3   [although, glucosesensing, neuron, identify, a...   \n",
       "4   [tomographic, reconstruction, image, quality, ...   \n",
       "..                                                ...   \n",
       "47  [many, essential, biological, process, control...   \n",
       "48  [protection, endothelial, integrity, recognize...   \n",
       "49  [organicinorganic, hybrid, perovskites, emerge...   \n",
       "50  [capture, humid, flue, gas, atmosphere, porous...   \n",
       "51  [during, intracellular, membrane, traffic, net...   \n",
       "\n",
       "                                          title_token  \n",
       "0                                          [win, heo]  \n",
       "1   [structure, dynamic, wettability, water, metal...  \n",
       "2   [design, broadband, solar, thermal, absorber, ...  \n",
       "3   [glucosesensing, neuron, pair, regulate, insul...  \n",
       "4   [deep, learningbased, optical, field, screen, ...  \n",
       "..                                                ...  \n",
       "47  [chemical, biology, route, sitespecific, authe...  \n",
       "48  [amelioration, sepsis, tie, activationinduced,...  \n",
       "49  [overcome, electroluminescence, efficiency, li...  \n",
       "50  [capture, humid, flue, gas, humid, atmosphere,...  \n",
       "51  [springloaded, unravel, single, snare, complex...  \n",
       "\n",
       "[681 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content</th>\n      <th>title</th>\n      <th>url</th>\n      <th>content_token</th>\n      <th>title_token</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>“Good ideas come to me when I am very comforta...</td>\n      <td>Won Do Heo</td>\n      <td>https://www.nature.com/articles/s41592-019-0626-1</td>\n      <td>[good, idea, comfortable, happy, win, heo, bio...</td>\n      <td>[win, heo]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The water/metal interface often governs import...</td>\n      <td>Structure, Dynamics, and Wettability of Water ...</td>\n      <td>https://www.nature.com/articles/s41598-019-513...</td>\n      <td>[the, watermetal, interface, govern, chemophys...</td>\n      <td>[structure, dynamic, wettability, water, metal...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>In using nanostructures to design solar therma...</td>\n      <td>Design of a Broadband Solar Thermal Absorber U...</td>\n      <td>https://www.nature.com/articles/s41598-019-514...</td>\n      <td>[use, nanostructures, design, solar, thermal, ...</td>\n      <td>[design, broadband, solar, thermal, absorber, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Although glucose-sensing neurons were identifi...</td>\n      <td>A glucose-sensing neuron pair regulates insuli...</td>\n      <td>https://www.nature.com/articles/s41586-019-1675-4</td>\n      <td>[although, glucosesensing, neuron, identify, a...</td>\n      <td>[glucosesensing, neuron, pair, regulate, insul...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>In tomographic reconstruction, the image quali...</td>\n      <td>Deep learning-based optical field screening fo...</td>\n      <td>https://www.nature.com/articles/s41598-019-513...</td>\n      <td>[tomographic, reconstruction, image, quality, ...</td>\n      <td>[deep, learningbased, optical, field, screen, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>Many essential biological processes are contro...</td>\n      <td>A chemical biology route to site-specific auth...</td>\n      <td>https://science.sciencemag.org/content/354/631...</td>\n      <td>[many, essential, biological, process, control...</td>\n      <td>[chemical, biology, route, sitespecific, authe...</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>Protection of endothelial integrity has been r...</td>\n      <td>Amelioration of sepsis by TIE2 activation–indu...</td>\n      <td>https://stm.sciencemag.org/content/8/335/335ra55</td>\n      <td>[protection, endothelial, integrity, recognize...</td>\n      <td>[amelioration, sepsis, tie, activationinduced,...</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>Organicinorganic hybrid perovskites are emergi...</td>\n      <td>Overcoming the electroluminescence efficiency ...</td>\n      <td>https://science.sciencemag.org/content/350/626...</td>\n      <td>[organicinorganic, hybrid, perovskites, emerge...</td>\n      <td>[overcome, electroluminescence, efficiency, li...</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>Capturing CO2 from humid flue gases and atmosp...</td>\n      <td>CO2 capture from humid flue gases and humid at...</td>\n      <td>https://science.sciencemag.org/content/350/625...</td>\n      <td>[capture, humid, flue, gas, atmosphere, porous...</td>\n      <td>[capture, humid, flue, gas, humid, atmosphere,...</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>During intracellular membrane trafficking Neth...</td>\n      <td>Spring-loaded unraveling of a single SNARE com...</td>\n      <td>https://science.sciencemag.org/content/347/622...</td>\n      <td>[during, intracellular, membrane, traffic, net...</td>\n      <td>[springloaded, unravel, single, snare, complex...</td>\n    </tr>\n  </tbody>\n</table>\n<p>681 rows × 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "journal_df = pd.DataFrame()\n",
    "for folder in os.listdir(join('journal_crawling')):\n",
    "    for file in os.listdir(join('journal_crawling',folder)):\n",
    "        if file.endswith('.csv'):\n",
    "            temp_df = pd.read_csv(join('journal_crawling',folder,file))\n",
    "            try:\n",
    "                temp_df.rename(columns={\"urls\" : \"url\",\"titles\":\"title\",\"contents\":\"content\"},inplace=True)\n",
    "                temp_df=temp_df.drop(['Unnamed: 0'],axis=1)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                temp_unscreen=visualize_english(temp_df,tokenize_nltk,'./visualize/journal_visualize/'+file.rstrip('.csv'))\n",
    "                journal_df = pd.concat([journal_df,temp_unscreen])\n",
    "            except:\n",
    "                break\n",
    "visualize_english(journal_df,None,'./visualize/journal_visualize/merge',keyword_num=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_tokenizer(text):\n",
    "    \"\"\"\n",
    "    sklearn의 Tfidfvectorizer를 이용함에 있어서 우리의 nori 토크나이저가 토큰화 한 결과를 사용하기 위한 method이다.\n",
    "\n",
    "    :param text: 토큰화 하고자 하는 문장\n",
    "    :return: 토큰이 담긴 리스트가 반환된다.\n",
    "    \"\"\"\n",
    "    list1 = text.split('00')\n",
    "    return list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TfidfWord(df):\n",
    "    #TF-IDF를 기반으로 카이스트 검색시 가장 의미가 높은 단어로 예상되는 단어 100가지를 추출함.\n",
    "\n",
    "    vectorizer = TfidfVectorizer(tokenizer=identity_tokenizer,\n",
    "                                    max_features=100,\n",
    "                                    max_df=0.5,\n",
    "                                    ngram_range=(1, 1))\n",
    "\n",
    "    # content를 기반으로 TF-IDF를 돌리는 함수\n",
    "    words_list_content = []\n",
    "    token_concat = df['content_token'].map(lambda x: \"00\".join(x))\n",
    "    tfidf_matrix = vectorizer.fit_transform(token_concat)\n",
    "    tfidf_wordslist = vectorizer.get_feature_names()\n",
    "    vocab = dict()\n",
    "    for idx, word in enumerate(tfidf_wordslist):\n",
    "        vocab[word] = tfidf_matrix.getcol(idx).sum()\n",
    "    words_list_content = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "    # title을 기반으로 TF-IDF를 돌리는 함수\n",
    "    words_list_title = []\n",
    "    token_concat = df['title_token'].map(lambda x: \"00\".join(x))\n",
    "    tfidf_matrix = vectorizer.fit_transform(token_concat)\n",
    "    tfidf_wordslist = vectorizer.get_feature_names()\n",
    "    vocab = dict()\n",
    "    for idx, word in enumerate(tfidf_wordslist):\n",
    "        vocab[word] = tfidf_matrix.getcol(idx).sum()\n",
    "    words_list_title = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return words_list_content, words_list_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountWord(df):\n",
    "    #단어가 나온 빈도수를 기반으로 카이스트 검색시 가장 의미가 높은 단어로 예상되는 단어 100가지를 추출함.\n",
    "\n",
    "    vectorizer = CountVectorizer(tokenizer=identity_tokenizer,\n",
    "                                    max_features=100,\n",
    "                                    max_df=0.5,\n",
    "                                    ngram_range=(1, 1))\n",
    "\n",
    "    # content를 기반으로 TF-IDF를 돌리는 함수\n",
    "    words_list_content = []\n",
    "    token_concat = df['content_token'].map(lambda x: \"00\".join(x))\n",
    "    tfidf_matrix = vectorizer.fit_transform(token_concat)\n",
    "    tfidf_wordslist = vectorizer.get_feature_names()\n",
    "    vocab = dict()\n",
    "    for idx, word in enumerate(tfidf_wordslist):\n",
    "        vocab[word] = tfidf_matrix.getcol(idx).sum()\n",
    "    words_list_content = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "    # title을 기반으로 를 돌리는 함수\n",
    "    words_list_title = []\n",
    "    token_concat = df['title_token'].map(lambda x: \"00\".join(x))\n",
    "    tfidf_matrix = vectorizer.fit_transform(token_concat)\n",
    "    tfidf_wordslist = vectorizer.get_feature_names()\n",
    "    vocab = dict()\n",
    "    for idx, word in enumerate(tfidf_wordslist):\n",
    "        vocab[word] = tfidf_matrix.getcol(idx).sum()\n",
    "    words_list_title = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return words_list_content, words_list_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Documents:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "    def __iter__(self):\n",
    "        with open(self.path, encoding='utf-8') as f:\n",
    "            for doc in f:\n",
    "                yield doc.strip().split()\n",
    "\n",
    "class Corpus:\n",
    "    def __init__(self, path, dictionary):\n",
    "        self.path = path\n",
    "        self.dictionary = dictionary\n",
    "        self.length = 0\n",
    "    def __iter__(self):\n",
    "        with open(self.path, encoding='utf-8') as f:\n",
    "            for doc in f:\n",
    "                yield self.dictionary.doc2bow(doc.split())\n",
    "    def __len__(self):\n",
    "        if self.length == 0:\n",
    "            with open(self.path, encoding='utf-8') as f:\n",
    "                for i, doc in enumerate(f):\n",
    "                    continue\n",
    "            self.length = i + 1\n",
    "        return self.length\n",
    "\n",
    "\n",
    "def topic_modeling(corpus_path,html_path):\n",
    "    documents = Documents(corpus_path)\n",
    "    dictionary = gensim.corpora.Dictionary(documents)\n",
    "    min_count = 5\n",
    "    word_counter = Counter((word for words in documents for word in words))\n",
    "    removal_word_idxs = {\n",
    "        dictionary.token2id[word] for word, count in word_counter.items()\n",
    "        if count < min_count\n",
    "    }\n",
    "\n",
    "    dictionary.filter_tokens(removal_word_idxs)\n",
    "    dictionary.compactify()\n",
    "    corpus = Corpus(corpus_path, dictionary)\n",
    "    lda_model = LdaModel(corpus, id2word=dictionary, num_topics=50)\n",
    "    prepared_data = gensimvis.prepare(lda_model, corpus, dictionary, mds='mmds')\n",
    "    pyLDAvis.save_html(prepared_data, html_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cooccur(tokens,target,window,num):\n",
    "    cooccur_dict = dict()\n",
    "    for token in tokens:\n",
    "        indices = [i for i, x in enumerate(token) if x.lower() == target.lower()]\n",
    "        if len(indices)!=0:\n",
    "            for indice in indices:\n",
    "                for i in np.arange(indice-window,indice+window):\n",
    "                    if i>=0 and i<=len(token)-1:\n",
    "                        if token[i] in cooccur_dict.keys() :\n",
    "                            cooccur_dict[token[i]] += 1\n",
    "                        else :\n",
    "                            cooccur_dict[token[i]] = 1 \n",
    "    try :\n",
    "        del cooccur_dict[target]\n",
    "    except KeyError:\n",
    "        try:\n",
    "            del cooccur_dict[target.upper()]\n",
    "        except KeyError:\n",
    "            print(target)\n",
    "    return sorted(cooccur_dict.items(),key=lambda item:item[1],reverse=True)[:num]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_remove(dict):\n",
    "    stopwords  = ['kaist','교수']\n",
    "    for stopword in stopwords:\n",
    "        try:\n",
    "            del dict[stopword]\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tuple2dict(tups):\n",
    "    target_dict =dict((x,int(y)) for x,y in tups)\n",
    "    for key in list(target_dict.keys()):\n",
    "        if len(key) ==1 :\n",
    "           del target_dict[key]\n",
    "    return target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dftotext(df,path):\n",
    "    textlist = df['content_token'].tolist()\n",
    "    with open(path,'w',encoding='utf-8-sig') as f:\n",
    "        for text in textlist:\n",
    "                f.write(' '.join(text)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_korean(df,tokenizer,root_path,topic_num=10,keyword_num=50,topn=10):\n",
    "    os.makedirs(root_path,exist_ok = True)\n",
    "    if tokenizer is not None :\n",
    "        print('Tokenize Start')\n",
    "        df = tokenizer(df)\n",
    "        print('Tokenize End')\n",
    "    detoken_path = join(root_path,'detokenize_text.txt')\n",
    "    dftotext(df,detoken_path)\n",
    "    print('detokenize end')\n",
    "    topic_modeling(detoken_path, join(root_path,'topic_model.html'))\n",
    "    print('topic model made')\n",
    "    remove(detoken_path)\n",
    "\n",
    "    #keyword extract part \n",
    "    TF_content_word, TF_title_word= TfidfWord(df)\n",
    "    Count_content_word, Count_title_word= CountWord(df)\n",
    "    # TF_content_word_dict=word_tuple2dict(TF_content_word[:keyword_num])\n",
    "    # TF_content_word_dict = stopwords_remove(TF_content_word_dict)\n",
    "    # TF_title_word_dict=word_tuple2dict(TF_title_word[:keyword_num])\n",
    "    # TF_title_word_dict = stopwords_remove(TF_title_word_dict)\n",
    "    Count_content_word_dict = word_tuple2dict(Count_content_word[:keyword_num])\n",
    "    Count_content_word_dict = stopwords_remove(Count_content_word_dict)\n",
    "    Count_title_word_dict = word_tuple2dict(Count_title_word[:keyword_num])\n",
    "    Count_title_word_dict = stopwords_remove(Count_title_word_dict)\n",
    "\n",
    "\n",
    "    dict_list = [Count_content_word_dict,Count_title_word_dict]\n",
    "    titles_token=df['title_token'].tolist()\n",
    "    contents_token=df['content_token'].tolist()\n",
    "    tokens = titles_token + contents_token\n",
    "\n",
    "    #window내에서 함께 나타난 다른 토큰들의 내림차순 및 유사어로 추측되는 것을 내림차순 정렬 그리고 저장\n",
    "    model = Word2Vec(sentences=tokens,size=300,window=5,min_count=3,workers=4, sg=0)\n",
    "    print('word2vec end')\n",
    "\n",
    "    for j,method_dict in enumerate(dict_list):\n",
    "        keyword_list = []\n",
    "        for keyword in method_dict.keys():\n",
    "            subkeyword_dict = dict()\n",
    "            subkeyword_dict['keyword'] = keyword \n",
    "            subkeyword_dict['score'] = method_dict[keyword]\n",
    "\n",
    "            tups = find_cooccur(tokens,keyword,4,topn)\n",
    "            cooccur_list =[]\n",
    "            for i,tup in enumerate(tups): \n",
    "                cooccur_list.append({'index':i+1, 'subkeyword':tup[0],'cooccur_num':tup[1]})\n",
    "            subkeyword_dict['cooccur'] = cooccur_list\n",
    "            try:\n",
    "                tups = model.wv.similar_by_word(keyword,topn=topn)\n",
    "            except :\n",
    "                try:\n",
    "                    tups = model.wv.similar_by_word(keyword.upper(),topn=topn)\n",
    "                except : \n",
    "                    continue\n",
    "            similar_list =[]\n",
    "            for i,tup in enumerate(tups): \n",
    "                similar_list.append({'index':i+1, 'subkeyword':tup[0],'cooccur_num':tup[1]})\n",
    "            subkeyword_dict['similar'] = similar_list\n",
    "\n",
    "            keyword_list.append(subkeyword_dict)\n",
    "\n",
    "\n",
    "        if j==0:\n",
    "            with open(join(root_path,'content_wordcloud'+'.html'), 'w', encoding='UTF-8-sig') as file:\n",
    "                file.write(korean_render(json.dumps(keyword_list, ensure_ascii=False)))\n",
    "        else:\n",
    "            with open(join(root_path,'title_wordcloud'+'.html'), 'w', encoding='UTF-8-sig') as file:\n",
    "                file.write(korean_render(json.dumps(keyword_list, ensure_ascii=False)))\n",
    "        print('made visualize file')\n",
    "    return df\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_english(df,tokenizer,root_path,topic_num=10,keyword_num=50,topn=10):\n",
    "    os.makedirs(root_path,exist_ok = True)\n",
    "    print('Tokenize Start')\n",
    "    if tokenizer is not None :\n",
    "        df = tokenizer(df)\n",
    "    print('tokenize End')\n",
    "    detoken_path = join(root_path,'detokenize_text.txt')\n",
    "    dftotext(df,detoken_path)\n",
    "    print('detokenize end')\n",
    "    topic_modeling(detoken_path, join(root_path,'topic_model.html'))\n",
    "    print('topic model made')\n",
    "    remove(detoken_path)\n",
    "\n",
    "    #keyword extract part \n",
    "    TF_content_word, TF_title_word= TfidfWord(df)\n",
    "    Count_content_word, Count_title_word= CountWord(df)\n",
    "    # TF_content_word_dict=word_tuple2dict(TF_content_word[:keyword_num])\n",
    "    # TF_content_word_dict = stopwords_remove(TF_content_word_dict)\n",
    "    # TF_title_word_dict=word_tuple2dict(TF_title_word[:keyword_num])\n",
    "    # TF_title_word_dict = stopwords_remove(TF_title_word_dict)\n",
    "    Count_content_word_dict=word_tuple2dict(Count_content_word[:keyword_num])\n",
    "    Count_content_word_dict = stopwords_remove(Count_content_word_dict)\n",
    "    Count_title_word_dict=word_tuple2dict(Count_title_word[:keyword_num])\n",
    "    Count_title_word_dict = stopwords_remove(Count_title_word_dict)\n",
    "\n",
    "\n",
    "    dict_list = [Count_content_word_dict,Count_title_word_dict]\n",
    "    titles_token=df['title_token'].tolist()\n",
    "    contents_token=df['content_token'].tolist()\n",
    "    tokens = titles_token + contents_token\n",
    "\n",
    "    #window내에서 함께 나타난 다른 토큰들의 내림차순 및 유사어로 추측되는 것을 내림차순 정렬 그리고 저장\n",
    "    model = Word2Vec(sentences=tokens,size=300,window=5,min_count=3,workers=4, sg=0)\n",
    "    print('word2vec end')\n",
    "\n",
    "    for j,method_dict in enumerate(dict_list):\n",
    "        keyword_list = []\n",
    "        for keyword in method_dict.keys():\n",
    "            subkeyword_dict = dict()\n",
    "            subkeyword_dict['keyword'] = keyword \n",
    "            subkeyword_dict['score'] = method_dict[keyword]\n",
    "\n",
    "            tups = find_cooccur(tokens,keyword,4,topn)\n",
    "            cooccur_list =[]\n",
    "            for i,tup in enumerate(tups): \n",
    "                cooccur_list.append({'index':i+1, 'subkeyword':tup[0],'cooccur_num':tup[1]})\n",
    "            subkeyword_dict['cooccur'] = cooccur_list\n",
    "            try:\n",
    "                tups = model.wv.similar_by_word(keyword,topn=topn)\n",
    "            except :\n",
    "                try:\n",
    "                    tups = model.wv.similar_by_word(keyword.upper(),topn=topn)\n",
    "                except : \n",
    "                    continue\n",
    "            similar_list =[]\n",
    "            for i,tup in enumerate(tups): \n",
    "                similar_list.append({'index':i+1, 'subkeyword':tup[0],'cooccur_num':tup[1]})\n",
    "            subkeyword_dict['similar'] = similar_list\n",
    "\n",
    "            keyword_list.append(subkeyword_dict)\n",
    "\n",
    "\n",
    "        if j==0:\n",
    "            with open(join(root_path,'content_wordcloud'+'.html'), 'w', encoding='UTF-8-sig') as file:\n",
    "                file.write(english_render(json.dumps(keyword_list, ensure_ascii=False)))\n",
    "        else:\n",
    "            with open(join(root_path,'title_worldcloud'+'.html'), 'w', encoding='UTF-8-sig') as file:\n",
    "                file.write(english_render(json.dumps(keyword_list, ensure_ascii=False)))\n",
    "        print('made visualize file')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.6.12 64-bit ('pytorch-CycleGAN-and-pix2pix': conda)",
   "display_name": "Python 3.6.12 64-bit ('pytorch-CycleGAN-and-pix2pix': conda)",
   "metadata": {
    "interpreter": {
     "hash": "4a65428369c2482fef3eeb880d3c90cbbd7b2b2d86e9d23bc2ecc7eb760c1319"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}